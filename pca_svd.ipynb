{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Closer Look | PCA, SVD & Eigendecomposition\n",
    "### Illustrating the connection between Principal Components Analysis, Singular Value Decomposition & Eigendecomposition\n",
    "\n",
    "#### Objective\n",
    "\n",
    "1. Walk through every step of PCA and show how it relates to singular value decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.68811868  1.26962382 -0.90187401 -1.43335751]\n",
      " [ 0.54329574  0.87531614 -1.47688597 -0.54723901]\n",
      " [-1.72325875  0.0507214   1.3888075  -0.79201964]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# generate random data\n",
    "np.random.seed(5)\n",
    "\n",
    "X, y = make_classification(n_samples=3, n_features=4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(X) = & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(x_i - \\bar{x}) \\\\\n",
    "= & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "$$\n",
    "Cov(x, y) = \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$$\n",
    "\n",
    "Note that Covariance can be thought of as \"variance with itself\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Cov(x, x) = & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(x_i - \\bar{x}) \\\\\n",
    "Cov(x, x) = & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})^2 = Var(x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "linear algebra notation for covariance:\n",
    "\n",
    "$$\n",
    "Cov(\\mathbf{X}) = \\frac{1}{N - 1}\\mathbf{X}^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "This results in a $p~x~p$ matrix where the $i^{th}$ diagonal element is the variance of the $i^{th}$ attribute of the original matrix, and all other elements $\\left( x_{i,j}~where~i\\neq j \\right)$ are the covariances of the $i^{th}$ and $j^{th}$ attributes in the original matrix.\n",
    "\n",
    "$$\n",
    "Cov(\\mathbf{X}) = \\frac{1}{N - 1}\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n",
    "                                                          \\sigma_1^2 & \\sigma_{1,2} & \\sigma_{1,3} & \\sigma_{1,4} \\\\\n",
    "                                                          \\sigma_{2,1} & \\sigma_2^2 & \\sigma_{2,3} & \\sigma_{2,4} \\\\\n",
    "                                                          \\sigma_{3,1} & \\sigma_{3,2} & \\sigma_3^2 & \\sigma_{3,4} \\\\\n",
    "                                                          \\sigma_{4,1} & \\sigma_{4,2} & \\sigma_{4,3} & \\sigma_4^2 \\\\\n",
    "                                                          \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Also, note that $x_{i,j}$ of the covariance matrix will be the same as $x_{j,i}$, since: \n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) \\right) = \\left( \\frac{1}{N - 1}\\sum_{i=1}^{N} (y_i - \\bar{y})(x_i - \\bar{x}) \\right)\n",
    "$$\n",
    "\n",
    "When one \"centers\" $\\mathbf{X}$ (by subtracting the column-wise mean from each data point, leading to a new $\\mathbf{X}$ where the column-wise means are equal to 0), the numerator of the covariance formula $\\left( \\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) \\right)$ becomes equivalent to the dot product (the sum of the element-wise products of two vectors).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Cov(x, y) = & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n",
    "= & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - 0)(y_i - 0) \\\\\n",
    "= & \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i)(y_i) \\\\\n",
    "Numerator~of~covariance = & \\sum_{i=1}^{N} x_iy_i = Dot~product\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn implementation of standardizing.\n",
      "=========================================================\n",
      "[[-0.07061706  1.05886006 -0.46200914 -1.36269391]\n",
      " [ 1.25852557  0.28242682 -0.92654053  1.00891219]\n",
      " [-1.18790851 -1.34128687  1.38854967  0.35378171]]\n",
      "\n",
      "NumPy implementation of standardizing.\n",
      "=========================================================\n",
      "[[-0.07061706  1.05886006 -0.46200914 -1.36269391]\n",
      " [ 1.25852557  0.28242682 -0.92654053  1.00891219]\n",
      " [-1.18790851 -1.34128687  1.38854967  0.35378171]]\n",
      "\n",
      "=========================================================\n",
      "=========================================================\n",
      "\n",
      "NumPy implementation of calculating the covariance matrix.\n",
      "=========================================================\n",
      "[[ 1.5         0.93699694 -1.3914596   0.47285546]\n",
      " [ 0.93699694  1.5        -1.30666318 -0.81624053]\n",
      " [-1.3914596  -1.30666318  1.5         0.09301124]\n",
      " [ 0.47285546 -0.81624053  0.09301124  1.5       ]]\n",
      "\n",
      "Manually calculating the covariance matrix.\n",
      "=========================================================\n",
      "[[ 1.5         0.93699694 -1.3914596   0.47285546]\n",
      " [ 0.93699694  1.5        -1.30666318 -0.81624053]\n",
      " [-1.3914596  -1.30666318  1.5         0.09301124]\n",
      " [ 0.47285546 -0.81624053  0.09301124  1.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn standardizing\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"\\nSklearn implementation of standardizing.\")\n",
    "print(\"=========================================================\")\n",
    "X_transformed = scaler.fit_transform(X)\n",
    "print(X_transformed)\n",
    "\n",
    "# numpy standardizing\n",
    "print(\"\\nNumPy implementation of standardizing.\")\n",
    "print(\"=========================================================\")\n",
    "X_centered = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "print(X_centered)\n",
    "\n",
    "\n",
    "print(\"\\n=========================================================\")\n",
    "print(\"=========================================================\")\n",
    "\n",
    "# numpy method for covariance matrix\n",
    "print(\"\\nNumPy implementation of calculating the covariance matrix.\")\n",
    "print(\"=========================================================\")\n",
    "cov_mat = np.cov(X_centered.T)\n",
    "print(cov_mat)\n",
    "\n",
    "# \"hard-coded\" version of calculating the covariance matrix\n",
    "print(\"\\nManually calculating the covariance matrix.\")\n",
    "print(\"=========================================================\")\n",
    "cov_mat1 = (1 / (X_centered.shape[0] - 1)) * (X_centered.T.dot(X_centered))\n",
    "print(cov_mat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.54143364 -0.56684159  0.61160043  0.10716878]\n",
      " [-0.40778653  0.33415932  0.0966321  -0.84422149]]\n",
      "[[-0.54143364 -0.56684159  0.61160043  0.10716878]\n",
      " [ 0.40778653 -0.33415932 -0.0966321   0.84422149]]\n"
     ]
    }
   ],
   "source": [
    "values, components = np.linalg.eig(cov_mat)\n",
    "values1, components1 = np.linalg.eig(cov_mat1)\n",
    "\n",
    "\n",
    "pca = PCA(svd_solver='full')\n",
    "pca.fit(X_centered)\n",
    "X_new = pca.transform(X_centered)\n",
    "\n",
    "# why are first two the only matching ones??\n",
    "print(pca.components_[:2,:])\n",
    "print(components.T[:2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99 -1.49  0.  ]\n",
      " [-1.3   1.36 -0.  ]\n",
      " [ 2.29  0.13  0.  ]]\n",
      "[[-0.99  1.49 -0.  ]\n",
      " [-1.3  -1.36  0.  ]\n",
      " [ 2.29 -0.13 -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# print(X_centered.shape)\n",
    "# print(components.shape)\n",
    "print(np.around(X_centered.dot(components)[:, :3], 2))\n",
    "print(np.around(X_new, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0c395741361a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# sklearn transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# sklearn transform\n",
    "x_train_1 = scaler.transform(x_train)\n",
    "x_test_1 = scaler.transform(x_test)\n",
    "\n",
    "# Numpy transform\n",
    "x_train_np = (x_train - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "x_test_np = (x_test - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "\n",
    "# sanity check ...\n",
    "# print(x_train_1 == x_train_np)\n",
    "\n",
    "# sklearn method\n",
    "pca = PCA(svd_solver='full')\n",
    "X_new = pca.fit_transform(X)\n",
    "# X_new = pca.transform(X)\n",
    "# print(np.around(pca.explained_variance_ratio_, 2))\n",
    "\n",
    "# numpy method using cov = (1/N)*X^T*X...\n",
    "cov = (1 / x_train_1.shape[0] - 1) * x_train_1.T.dot(x_train_1)\n",
    "values, components = np.linalg.eig(cov)\n",
    "\n",
    "# or just create the covariance matrix yourself...\n",
    "cov_mat = np.cov(x_train_1.T)\n",
    "values_1, components_1 = np.linalg.eig(cov_mat)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"PCA via NumPy: Manual Covariance Matrix\")\n",
    "print(\"=========================================================\")\n",
    "print(np.around(components.T, 4))\n",
    "print(\"\\n\")\n",
    "print(\"PCA via Sklearn\")\n",
    "print(\"=========================================================\")\n",
    "print(np.around(pca.components_.T, 4))\n",
    "print(\"\\n\")\n",
    "print(\"PCA via NumPy: NumPy-Created Covariance Matrix\")\n",
    "print(\"=========================================================\")\n",
    "print(np.around(components_1.T, 4))\n",
    "\n",
    "# SVD demo\n",
    "X = np.array([[1,1,1,0,0],\n",
    "              [3,3,3,0,0],\n",
    "              [4,4,4,0,0],\n",
    "              [5,5,5,0,0],\n",
    "              [0,2,0,4,4],\n",
    "              [0,0,0,5,5],\n",
    "              [0,1,0,2,2]])\n",
    "\n",
    "U, D, V = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"PCA via NumPy SVD\")\n",
    "print(\"=========================================================\")\n",
    "print(np.around(U.dot(np.diag(D)), 2))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"PCA via Sklearn\")\n",
    "print(\"=========================================================\")\n",
    "print(np.around(X_new, 2))\n",
    "\n",
    "# print(U)\n",
    "# print(np.diag(D))\n",
    "# print(V)\n",
    "\n",
    "U = U[:, :3]\n",
    "D = np.diag(D)[:3, :3]\n",
    "V = V[:3,]\n",
    "\n",
    "# print(np.around(U, 2))\n",
    "# print(np.around(np.diag(D), 2))\n",
    "# print(np.around(V, 2))\n",
    "\n",
    "# svd reconstruction demo\n",
    "u_new = U[:,:-1]\n",
    "d_new = D[:2, :2]\n",
    "v_new = V[:-1, :]\n",
    "\n",
    "reconstructed = np.matmul(u_new.dot(d_new), v_new)\n",
    "\n",
    "# little loss of data\n",
    "# print(np.around(reconstructed, 2))\n",
    "# print(\"\\n\")\n",
    "# print(np.around(X, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
